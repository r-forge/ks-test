\documentclass[article]{jss}

\usepackage{amsthm, amsmath, amssymb}
%\usepackage{Sweave}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% almost as usual
\author{Taylor B. Arnold \\ Yale University \And 
        John W. Emerson\\ Yale University}
\title{Nonparametric Goodness-of-Fit Tests \\ for Discrete Null Distributions}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Taylor B. Arnold, John W. Emerson} %% comma-separated
\Plaintitle{Nonparametric Goodness-of-Fit Tests for Discrete Distributions} %% without formatting
\Shorttitle{Nonparametric Goodness-of-Fit Tests for Discrete Distributions} %% a short title (if necessary)

%% an abstract and keywords
\Abstract{
   A general theory for extending nonparametric goodness-of-fit tests to discrete null distributions has existed
   for several decades. Despite this, modern statistical software have generally failed to provide this methodology
   to users. The packages \pkg{ks.test} and \pkg{cvm.test} serve to fill this need in the \proglang{R} language for
   the two most popular nonparametric tests. The structure of these two packages are explained and examples
   to their specific usage are presented. Particular attention is given to the various numerical issues that arise
   in their implementation.
}
\Keywords{Cramer von Mises, goodness-of-fit tests, Kolmogorov-Smirnov, \proglang{R}}
\Plainkeywords{Cramer von Mises, goodness-of-fit tests, Kolmogorov-Smirnov, R} %% without formatting
%% at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{13}
%% \Issue{9}
%% \Month{September}
%% \Year{2004}
%% \Submitdate{2004-09-29}
%% \Acceptdate{2004-09-29}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Taylor B. Arnold \\
  24 Hillhouse Ave. \\
  New Haven, CT 06511, USA\\
  E-mail: \email{taylor.arnold@yale.edu}\\
  URL: \url{http://euler.stat.yale.edu/~tba3}\\


  John W. Emerson \\
  24 Hillhouse Ave. \\
  New Haven, CT 06511, USA\\
  E-mail: \email{john.emerson@yale.edu}\\
  URL: \url{http://euler.stat.yale.edu/~jay}
}
%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/1/31336-5053
%% Fax: +43/1/31336-734

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

\setkeys{Gin}{width=1.0\textwidth, height=1.0\textheight}


%% include your article here, just as usual
%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.

\section{Introduction}

Example code:
\begin{Schunk}
\begin{Sinput}
> 1 + 1
\end{Sinput}
\begin{Soutput}
[1] 2
\end{Soutput}
\end{Schunk}

%General idea of goodness-of-fit tests (hypothesis test about underlying distribution).
%Given an observed sequence of data, the typical question of statistical inference is to determine
%the underlying process from which it came. 
Given a particular null distribution, goodness-of-fit tests 
are used in order to test whether it is likely that the data were generated via the null distribution.

%Why we use non-parametric tests (power to interesting shifts; robustness to underlying assumptions).
While almost any hypothesis test can be viewed as a variant on a goodness-of-fit test, the term is typically
applied to those tests which are nonparametric in nature. That is, they do not conduct the hypothesis testing 
through the explicit calculation of the parameters generating the data. Instead, they generally attempt to 
calculate differences between the empirical distribution of the observed data and the overall distribution 
of the null model. In many cases these tests are preferential since they tend to have an increased power for 
interesting deviations from the null model, in exchange for failing to detect less interesting deviations due to 
factors such as measurement error. By far the most popular of these nonparametric tests are due to Kolmogorov and 
Smirnov, followed closely by several variants on an estimation procedure proposed by Cramer von Mises. 

% Discrete versions exist; just no code
While the original aim of nonparametric goodness of fit tests was meant for continuous null distributions, discrete versions
have existed since the early 1970s. Unfortunately, current statistical software has largely failed to incorporate them.
This has left the end-user to either use only parametric tests, such as Pearson's Chi-Squared test, or to incorrectly
apply the functions meant for continuous distributions. As shown in section 5, the latter can be particularly dangerous
in the small sample setting. The packages \pkg{ks.test} and \pkg{cvm.test} serve to fill this void for users
operating in the \proglang{R} environment. 

\section{Kolmogorov-Smirnov Test}

% Most popular choice of test; explain the basic algorithm and purpose for continuous distributions (maybe a few P's).

\subsection{Overview}

Of all the methods for nonparametric goodness-of-fit tests, by far the most popular is the method devised by Kolmogorov and Smirnov.
It is the only such test built into the base of \proglang{R}. The idea behind it is fairly simple. Given the cumulative distribution
function $F_0(x)$ of the continuous null distribution, and the empirical distribution function $F_{data}(x)$ of the observed data, 
one constructs the statistic:
\begin{align}
D = \sup_x \left| F_0(x)- F_{data}(x) \right|
\end{align}
The distribution of $D$ under the null model does not depend on which null distribution is being used, making this a computationally
attractive method. For a standard treatment of the test and its performance relative to other algorithms see \cite{slakter65}. Two common
alternatives of the above test statistic exist. The absolute value is discarded and the rest is either left alone (the `greater' testing 
alternative) or the supremum is replaces with a negative infimum (the `lesser' hypothesis alternative). These can be helpful depending 
on the nature of the alternative hypotheses for which the test is desired to be powerful towards.

The extension of this result to non-continuous null distributions does not have such a clean solution. The formula of
the test statistic $D$ remains unchanged, however the distribution of the testing statistic is much more difficult. Unlike in the 
continuous case, it ultimately depends on which null model was chosen; this makes it impossible to simply read p-values directly 
off of a fixed table. It was known since at least the 1950's that using the tables for continuous distributions resulted in 
conservative p-values; it was not until \citet{Conover1972} that a method for computing exact p-values in this case was developed.

\subsection{Implementation}

The implementation of the discrete Kolmogorov-Smirnov function consists of two parts. First the particular test statistic needs to be calculated,
and then the p-value for that particular statistic must be computed. 

Given that the test statistic is, theoretically, the same as in the
continuous case it would seem that the first part could be directly taken from the existing procedures. This is, unfortunately, often not the case.
Consider two non-decreasing functions $f$ and $g$, where the function $f$ is a step function with jumps on the set $\{x_1, \ldots x_N \}$ and $g$
is continuous. If we want to determine the supreme of the difference between these two functions notice that:

\begin{align}
\sup_x \left| f(x)- g(x) \right| &= \max_i \left( \left|g(x_i) - f(x_i) \right|, \lim_{x \rightarrow x_i} \left| g(x) - f(x_{i-1}) \right| \right) \\
&=  \max_i \left( \left|g(x_i) - f(x_i) \right|, \left| g(x_i) - f(x_{i-1}) \right| \right)
\end{align}

Computing the maximum over these $2N$ values (with $f$ equal to $F_{data}(x)$ and $g$ equal to $F_0(x)$ as defined above) is clearly the 
most efficient way to compute the Kolmogorov-Smirnov test statistic when given a continuous null distribution. When the function $g$ is not
continuous, notice that this formula no longer works since in general we cannot replace the limit of $g$ with its value at $x_i$. If it is known that $g$
is also a step function, we could replace the formula for some small $\epsilon$ by:
\begin{align}
\sup_x \left| f(x)- g(x) \right| &=  \max_i \left( \left|g(x_i) - f(x_i) \right|, \left| g(x_i - \epsilon) - f(x_{i-1}) \right| \right)
\end{align}
Where the discontinuities in $g$ are at least a distance $\epsilon$ apart. 

%Once the test statistic is determined, the p-value for this value needs to be computed. For high sample sizes, the test distribution becomes the same as in the
%continuous case. The default options for \pkg{ks.test} already allow the user to specify if they would like approximate asymptotic distributions (since these exist
%in the continuous setting as well). So when this approximation flag is turned on, this distribution is used for all situations. When the user requires exact p-values,
%the methodology in \citet{Conover1972} needs to be used. Code for this procedure in the \proglang{R} language, or in any other open source options, did not previously
%exist and is included in the revised \pkg{ks.test}. The calculations are complex but straightforward; the full details are contained in the package source and in 
%the original Conover paper. 

%Numerical issues arise from the summation; need to do something else for high $n$. Simulation? Asymptotic approximation? 
%The interesting part of the implementation of Conover's method, from a computational standpoint, are the difficult numerical issues that arise when calculated the p-values for larger sample sizes. 

\section{Cramer von Mises Tests}

\subsection{Algorithm}

%General idea for continuous distributions. In many cases actually more powerful of the two. There again exists a distribution free 
%distribution for the testing statistic.
While the Kolmogorov-Smirnov test is the most well known of the non-parametric goodness of fit tests, there is another family of 
tests which has been shown to be more powerful to a large class of alternatives distributions. The original was developed jointly by
Harald Cramer and Richard von Mises \citep{cramer1928, vonmises1928}, and further adapted by \cite{anderson1952}, and 
\cite{Watson1961}. The test statistics are, respectively, given as:
\begin{align*}
W^2 &= n \cdot \int_{-\infty}^{\infty} \left[ F_{data}(x)- F_{0}(x) \right]^2 dF_0(x) \\
A^2 &= n \cdot \int_{-\infty}^{\infty} \left[F_0(x) -F_0(x)^2 \right]^{-1} \left[ F_{data}(x)- F_{0}(x) \right]^2 dF_0(x) \\
U^2 &= n \cdot \int_{-\infty}^{\infty} \left[ F_{data}(x)- F_{0}(x) - W^2 \right]^2 dF_0(x)
\end{align*}
Where $F$ is either the cumulative distribution of the null model or the empirical cumulative distribution of the observed data.
As in the Kolmogorov-Smirnov test statistic, these all have distribution free null distributions in the continuous case. 

It has been shown that these tests can be more powerful than Kolmogorov-Smirnov tests to certain deviations. As they all involve
integration over the whole range of data, rather than one supremum, it is not surprising that they are generally best when the
true alternative distribution deviates a little over the whole range of data rather than deviating a lot over a small range. For
a complete analysis of the relative powers of these tests see \cite{stephens1974}.

%Discuss the discrete version. How to calculate W2, U2, and A2. 
Generalizations of the Cramer-von Mises tests were developed in \cite{choulakian1994}. Much like for the Kolmogorov-Smirnov test,
the theoretical form of the test statistics are unchanged; although the discreteness allows for a slightly simpler representation.
The null distribution of the test statistics are again distribution dependent, unlike the continuous version. The methods do not
suggest finite sample results, but rather show that the asymptotic null distribution is equal to a weighted sum of independent
chi-squared variables (the weights depending on the particular distribution). This asymptotic distribution is what we implement here;
the original papers shows that this approximation 

\subsection{Implementation}

For the user, the function \pkg{cvm.test} was designed to work in exactly the same manner as \pkg{ks.test} in \proglang{R}. This
alleviates the need to learn a new set of input and output instructions and hopefully will increase the usage of these tests. 
As there is no default functionality in \proglang{R} for continuous Cramer-von Mises, subsequent versions of the package
should also originally implement the continuous case even though this is not what we concentrate on here. 

Calculation of the three test statistics is done by straightforward matrix algebra as given in \cite{choulakian1994}. Determining 
the form of the asymptotic null distribution is also easy using the built-in eigenvalue decomposition functions. The only 
difficulty in the process involves actually calculating the percentiles for these weighted chi-squares. 

%Show how to compute the distribution of a weighted sum of chi squared's.
The method used for calculating the distribution of a weighted sum of independent chi-squared variables is given in \cite{imhof1961}.
A general method for computing any quadratic form of normals is presented, which is easily adapted for our case since
each chi-squared variable has only one degree of freedom. The exact formula given for the distribution function of $Q$, the weighted
sum of chi squares, is:
\begin{align*}
\mathbb{P}\{Q \geq x \} = \frac{1}{2} + \frac{1}{\pi} \int_{0}^{\infty} \frac{\sin\theta(u,x)}{u \rho(u) } du
\end{align*}
For continuous functions $\theta(\cdot, x)$ and $\rho(\cdot)$ which depend on the actual weights used. 

There is no analytic solution to the integral, but integration can be carried out using numerical techniques. This seems
fine in most situations, but numerical issues do become a problem in the regime of large test statistics $x$. The function
$\theta(\cdot, x)$ is linear in $x$, and thus as the the test statistic grows the corresponding period of the integrand 
decreases. As the function acquires too many inflection points, the approximation becomes unstable. This is further
magnified by this occurring when $p$-values should be very small; thus tiny fluctuations which would be undetectable
elsewhere are quite prominent. Figure ? shows the non-monotonicity of the function as the test statistic grows.

There is fortunately a simple conservative approximation which can get around this numerical problem. Given the
following inequality:
\begin{align*}
\mathbb{P} \left(\sum_{i=1}^{p} \lambda_i \chi^2_1 \geq x \right) &\leq \mathbb{P} \left( \lambda_{max} \sum_{i=1}^{p} \chi^2_1 \geq x \right) \\
&= \mathbb{P} \left(\chi^2_p \geq \frac{x}{p \, \lambda_{max}} \right)
\end{align*}
We see that the values for the weighted sum can be bounded by a simple transformation and a chi-squared distribution
of a higher degree of freedom. 

\section{Discussion}

Go into more detail about reasons for using these versus chi-squared type tests. 

Issues revolving around incorrect usage of default ks.test() in R because of continuity error (i.e. ks.test(1:2, ecdf(1:2))

Further discuss numerical issues; ks -> n over 30 has problems, CvM -> integrate function can sometimes refuse to run because of failure to calculate integral.

Benefits and drawbacks of using simulation to gather p-values. 

Possibly go into a brief discussion of classes and class inheritance in R. The use of `htest' in our functions; benefits and issues with using it. Particularly troublesome in ks.test where we have a range of p-values.

Two-sample cases; no theory for discrete distributions. Why? Distribution dependence messes things up. Discuss this for sure.

For further generalizations of tests see \cite{dewev1973}

\section{Examples}


%\bibliographystyle{plain}
\bibliography{jss}

\end{document}



